---
date: "2024-09-21T17:17:41+03:00"
description: ""
id: 8nrz4spinaeo5317xot4w74
publish: true
title: Linear Regression
updated: 1727203598152
---

<https://ru.wikipedia.org/wiki/Метод_наименьших_квадратов>

<https://en.wikipedia.org/wiki/Residual_sum_of_squares>

## Simple Linear Regression

Функция, которая пытается описать набор эксперементальных данных, изображнных на плоскости.

$$\hat{Y}(x) = a_0 + a_1x$$

Для рассчета линейной регрессии требуется выбрать функцию ошибок, которая будет минимизироваться (ОНА НЕ ОБЯЗАТЕЛЬНО ДОЛЖНА ПРИЙТИ К 0).

Лучше всего брать функцию [[organize.resource.data-mining.mse|mse]], поскольку она сильно штрафует за ошибки. 

$$MSE (Mean Square Error)  = \frac{\sum_{i=1}^n{\left(Y - \hat{Y}\right)^2}}{n} \rArr min$$

Подставляем на место $\hat{Y}(x)$

$$MSE (Mean Square Error)  = \frac{\sum_{i=1}^n{\left(Y_i - a_0 - a_1x_i\right)^2}}{n} \rArr min$$

Для минимизации функции требуется найти градиент функции и двигаться в противоположном ему направлении. 

Используем [[organize.resource.data-mining.nabla|nabla]], по переменным $a_0$ и $a_1$, чтобы найти градиент.

$$\nabla MSE = \left(\frac{\sum_{i=1}^n{2\left(Y_i - a_0 - a_1x_i\right)(-1)}}{n}, \frac{\sum_{i=1}^n{2\left(Y_i - a_0 - a_1x_i\right)(-x_i)}}{n} \right)$$

Обобщим для $\hat{Y}(x)$

$$\nabla MSE = \left(-2\frac{\sum_{i=1}^n{\left(Y_i - \hat{Y}\right)}}{n}, -2\frac{\sum_{i=1}^n{\left(Y_i - \hat{Y}\right)x_i}}{n} \right)$$

Регрессия движется так

$$a_0^{(t+1)} = a_0^{(t)} - \nabla(MSE)_0$$

$$a_1^{(t+1)} = a_1^{(t)} - \nabla(MSE)_1$$


## Multiple Linear Regression

Функция, которая пытается описать зависимость таргета от N параметров.

$$\hat{Y}(x_1, x_2, ..., x_n) = a_0 + a_1x_1 + a_2x_2 + ... + a_nx_n$$

Суть в общем от этого не меняется.

$$MSE (Mean Square Error)  = \frac{\sum_{i=1}^n{\left(Y_i - x_i @ A\right)^2}}{n} \rArr min$$

- $@$ - матричное умножение (как в numpy)
- $Y_i$ - $i$ результат функции $Y$
- $A$ - вертикальный вектор из коэффициентов $a$ в функции $\hat{Y}(x)$
- $x_i$ - строка из $X$ значений в функции $\hat{Y}(x)$

Ищем градиент по $A$ переменным.

Записываем все в матричной форме для удобства

$$MSE = \frac{1}{n}(Y-X@A)^T(Y-X@A)$$

Вычисляем наблу.

$$\nabla MSE = \frac{2}{n}(-X)^T(Y-X@A)$$

вынесем, что выносится, получаем

$$\nabla MSE = -\frac{2}{n}X^T(Y-X@A)$$

_Честно говоря я пока не понял, как это преобразование получилось, нужно более подробно расписать. 
Я думаю, что тут разолжили квадрат и транспонировали одну из частей, чтобы умножение матриц происходило, а потом от одной из частей взяли дифференциал, но нужно посмотреть формулы более подробно._

Сама регрессия будет считаться так:

$$A_{начальные} = случайные числа.$$

$$A^{(t+1)} = A^{(t)} - \nabla(A^{(t)}) + \alpha$$

Еще формулы

$$y = X@A + \alpha$$

- $\alpha$ это погрешность

$$\hat{y} = X@A$$

Вектор оценок коэффициентов линейной регрессии.
Он используется для оценки корреляции между параметрами и оценки значимости параметров.

$$\hat{A} = (X^{T}X)^{(-1)}X^{T}Y$$

---

Суть проиходящего в том, что  мы пытаемся найти сумму квадратов отклонений аппроксимируемой функции от эксперементальных данных, потом мы ищем от этого градиент, чтобы это все минимизировать движемся по градиенту обратно, смещая коэффициенты. в качестве начальных значений мы ебашим случайные числа.

## Оценка качества модели

Точность предсказания оценивается при помощи таких функций как:

- [[organize.resource.data-mining.mae|mae]]
- [[organize.resource.data-mining.mape|mape]]

В некоторых библиотеках и материалах используется [[organize.resource.data-mining.coefficient-of-determination|coefficient of determination]] для оценки качества ошибки, но ее сложно интерпретировать, поэтому лучше использовать MAE.